{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hadoop.apache.org/hadoop-logo.jpg\" align =\"left\">\n",
    "<h1><div class=\"alert alert-block alert-info\">\n",
    "Yet Another Resource Negotiator (YARN)\n",
    "</div></h1>\n",
    "<img src=\"https://i.ytimg.com/vi/ZFbkNY6Xn94/maxresdefault.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hadoop.apache.org/hadoop-logo.jpg\" align =\"left\">\n",
    "<h1><div class=\"alert alert-block alert-info\">\n",
    "Hadoop MapReduce\n",
    "</div></h1>\n",
    "\n",
    "Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/199Q1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color ='blue'>Practice in YARN and MapReduce</font></h1>\n",
    "\n",
    "<h3>Part I: run the mapper and the reducer to obtain the word count of a input file</h3>\n",
    "\n",
    "<b>- Transfering the program files to the edge:</b>\n",
    "\n",
    "scp *.py e.luizfonseca-dsti@edge-1.au.adaltas.cloud:\n",
    "\n",
    "<i>Info: here I was in the directory where the files I wanted to transfer were saved, if it's not the case give the path.</i>\n",
    "\n",
    "The python files were given by Adaltas and are stored in Moodle.\n",
    "\n",
    "<b>- Organizing the directories</b>\n",
    "\n",
    "hdfs dfs -mkdir mr\n",
    "hdfs dfs -mv raw/1342-0.txt raw/input.txt\n",
    "\n",
    "<b>- Running the program</b>\n",
    "\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\ \n",
    "\n",
    "    -file mapper.py -mapper \"python mapper.py\" \\\n",
    "    \n",
    "    -file reducer.py -reducer \"python reducer.py\" \\\n",
    "    \n",
    "    -input raw/input.txt \\\n",
    "    \n",
    "     -output mr/output\n",
    "     \n",
    "<b>- Reading the result file</b> \n",
    "\n",
    "hdfs dfs -cat mr/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part II: create a mapper and reducer script that will take the output of the wordcount as input, and output the most frequent word.</h3>\n",
    "\n",
    "<b>- Developing the mapper and the reducer </b>\n",
    "\n",
    "As we got as input the file with the counting the only task is to find the word with highest frequency, the algorithm is the same in the map and in the reduce stage, the difference is that during map we are distributing the task so we can have many files and in the reduce phase our job is to search amongst the files from the previous step the one that contains the most frequent word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_word = None\n",
    "highest_frequency_word = None\n",
    "highest_frequency_count = 0\n",
    "word = None\n",
    "\n",
    "#input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    if count > highest_frequency_count:\n",
    "        highest_frequency_count = count\n",
    "        highest_frequency_word = word\n",
    "\n",
    "#print on the file the first word with the highest frequency\n",
    "print '%s\\t%s' % (highest_frequency_word, highest_frequency_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>- Running the program</b>\n",
    "\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "\n",
    "-file mapper.py -mapper \"python mapper_frequency.py\" \\\n",
    "\n",
    "-file reducer.py -reducer \"python mapper_frequency.py\" \\\n",
    "\n",
    "-input raw/part-00000 \\\n",
    "\n",
    " -output mr_freq/output\n",
    " \n",
    "<b>- Reading the result file</b> \n",
    "\n",
    "hdfs dfs -cat mr_freq/output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
